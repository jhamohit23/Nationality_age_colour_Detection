{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e956e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76d64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_dataset(zip_path, extract_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a9738f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unzip_dataset() missing 1 required positional argument: 'extract_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Unzip UTKFace dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43munzip_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/multitask_prediction/datasets/UTKFace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: unzip_dataset() missing 1 required positional argument: 'extract_path'"
     ]
    }
   ],
   "source": [
    "# Unzip UTKFace dataset\n",
    "unzip_dataset('C:/Users/Dell/OneDrive/Desktop/Nationality_detection/source_data/archive (7).zip', 'datasets/UTKFace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d285e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip FER-2013 dataset\n",
    "unzip_dataset('C:/Users/Dell/OneDrive/Desktop/Nationality_detection/source_data/archive (8).zip', 'datasets/FER-2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99d83b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets unzipped successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Datasets unzipped successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df423935",
   "metadata": {},
   "source": [
    "# Organize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1032a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3f401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "954ba9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f85698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking contents of datasets/UTKFace:\n",
      "  Total files/directories: 3\n",
      "  First few items: ['crop_part1', 'UTKFace', 'utkface_aligned_cropped']\n",
      "Checking contents of datasets/FER-2013:\n",
      "  Total files/directories: 2\n",
      "  First few items: ['test', 'train']\n"
     ]
    }
   ],
   "source": [
    "def check_directory_contents(directory):\n",
    "    print(f\"Checking contents of {directory}:\")\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"  Directory does not exist!\")\n",
    "        return\n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    print(f\"  Total files/directories: {len(files)}\")\n",
    "    print(f\"  First few items: {files[:5]}\")\n",
    "\n",
    "check_directory_contents('datasets/UTKFace')\n",
    "check_directory_contents('datasets/FER-2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f77958d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking contents of datasets/UTKFace:\n",
      "  Total files/directories: 3\n",
      "  First few items: ['crop_part1', 'UTKFace', 'utkface_aligned_cropped']\n",
      "  Checking contents of datasets/UTKFace\\crop_part1:\n",
      "    Total files/directories: 9780\n",
      "    First few items: ['100_1_0_20170110183726390.jpg.chip.jpg', '100_1_2_20170105174847679.jpg.chip.jpg', '101_1_2_20170105174739309.jpg.chip.jpg', '10_0_0_20161220222308131.jpg.chip.jpg', '10_0_0_20170103200329407.jpg.chip.jpg']\n",
      "  Checking contents of datasets/UTKFace\\UTKFace:\n",
      "    Total files/directories: 23708\n",
      "    First few items: ['100_0_0_20170112213500903.jpg.chip.jpg', '100_0_0_20170112215240346.jpg.chip.jpg', '100_1_0_20170110183726390.jpg.chip.jpg', '100_1_0_20170112213001988.jpg.chip.jpg', '100_1_0_20170112213303693.jpg.chip.jpg']\n",
      "  Checking contents of datasets/UTKFace\\utkface_aligned_cropped:\n",
      "    Total files/directories: 2\n",
      "    First few items: ['crop_part1', 'UTKFace']\n",
      "    Checking contents of datasets/UTKFace\\utkface_aligned_cropped\\crop_part1:\n",
      "      Total files/directories: 9780\n",
      "      First few items: ['100_1_0_20170110183726390.jpg.chip.jpg', '100_1_2_20170105174847679.jpg.chip.jpg', '101_1_2_20170105174739309.jpg.chip.jpg', '10_0_0_20161220222308131.jpg.chip.jpg', '10_0_0_20170103200329407.jpg.chip.jpg']\n",
      "    Checking contents of datasets/UTKFace\\utkface_aligned_cropped\\UTKFace:\n",
      "      Total files/directories: 23708\n",
      "      First few items: ['100_0_0_20170112213500903.jpg.chip.jpg', '100_0_0_20170112215240346.jpg.chip.jpg', '100_1_0_20170110183726390.jpg.chip.jpg', '100_1_0_20170112213001988.jpg.chip.jpg', '100_1_0_20170112213303693.jpg.chip.jpg']\n",
      "\n",
      "\n",
      "Checking contents of datasets/FER-2013:\n",
      "  Total files/directories: 2\n",
      "  First few items: ['test', 'train']\n",
      "  Checking contents of datasets/FER-2013\\test:\n",
      "    Total files/directories: 7\n",
      "    First few items: ['angry', 'disgust', 'fear', 'happy', 'neutral']\n",
      "    Checking contents of datasets/FER-2013\\test\\angry:\n",
      "      Total files/directories: 958\n",
      "      First few items: ['PrivateTest_10131363.jpg', 'PrivateTest_10304478.jpg', 'PrivateTest_1054527.jpg', 'PrivateTest_10590091.jpg', 'PrivateTest_1109992.jpg']\n",
      "    Checking contents of datasets/FER-2013\\test\\disgust:\n",
      "      Total files/directories: 111\n",
      "      First few items: ['PrivateTest_11895083.jpg', 'PrivateTest_19671520.jpg', 'PrivateTest_21629266.jpg', 'PrivateTest_22382996.jpg', 'PrivateTest_26306320.jpg']\n",
      "    Checking contents of datasets/FER-2013\\test\\fear:\n",
      "      Total files/directories: 1024\n",
      "      First few items: ['PrivateTest_10153550.jpg', 'PrivateTest_10254684.jpg', 'PrivateTest_10306709.jpg', 'PrivateTest_10555537.jpg', 'PrivateTest_10629254.jpg']\n",
      "    Checking contents of datasets/FER-2013\\test\\happy:\n",
      "      Total files/directories: 1774\n",
      "      First few items: ['PrivateTest_10077120.jpg', 'PrivateTest_10470092.jpg', 'PrivateTest_10513598.jpg', 'PrivateTest_10516065.jpg', 'PrivateTest_10613684.jpg']\n",
      "    Checking contents of datasets/FER-2013\\test\\neutral:\n",
      "      Total files/directories: 1233\n",
      "      First few items: ['PrivateTest_10086748.jpg', 'PrivateTest_10767287.jpg', 'PrivateTest_11123843.jpg', 'PrivateTest_11164800.jpg', 'PrivateTest_11239107.jpg']\n",
      "  Checking contents of datasets/FER-2013\\train:\n",
      "    Total files/directories: 7\n",
      "    First few items: ['angry', 'disgust', 'fear', 'happy', 'neutral']\n",
      "    Checking contents of datasets/FER-2013\\train\\angry:\n",
      "      Total files/directories: 3995\n",
      "      First few items: ['Training_10118481.jpg', 'Training_10120469.jpg', 'Training_10131352.jpg', 'Training_10161559.jpg', 'Training_1021836.jpg']\n",
      "    Checking contents of datasets/FER-2013\\train\\disgust:\n",
      "      Total files/directories: 436\n",
      "      First few items: ['Training_10371709.jpg', 'Training_10598340.jpg', 'Training_1070239.jpg', 'Training_11050021.jpg', 'Training_11550217.jpg']\n",
      "    Checking contents of datasets/FER-2013\\train\\fear:\n",
      "      Total files/directories: 4097\n",
      "      First few items: ['Training_10018621.jpg', 'Training_10031494.jpg', 'Training_10110501.jpg', 'Training_10117992.jpg', 'Training_10126156.jpg']\n",
      "    Checking contents of datasets/FER-2013\\train\\happy:\n",
      "      Total files/directories: 7215\n",
      "      First few items: ['Training_10019449.jpg', 'Training_10046809.jpg', 'Training_10066226.jpg', 'Training_10070997.jpg', 'Training_10080933.jpg']\n",
      "    Checking contents of datasets/FER-2013\\train\\neutral:\n",
      "      Total files/directories: 4965\n",
      "      First few items: ['Training_10002154.jpg', 'Training_10031781.jpg', 'Training_10055498.jpg', 'Training_10059941.jpg', 'Training_10078021.jpg']\n"
     ]
    }
   ],
   "source": [
    "def check_directory_contents(directory, depth=0):\n",
    "    print(\"  \" * depth + f\"Checking contents of {directory}:\")\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"  \" * (depth+1) + \"Directory does not exist!\")\n",
    "        return\n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    print(\"  \" * (depth+1) + f\"Total files/directories: {len(files)}\")\n",
    "    print(\"  \" * (depth+1) + f\"First few items: {files[:5]}\")\n",
    "    \n",
    "    if depth < 2:  # Limit the depth to avoid excessive output\n",
    "        for item in files[:5]:\n",
    "            item_path = os.path.join(directory, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                check_directory_contents(item_path, depth + 1)\n",
    "\n",
    "check_directory_contents('datasets/UTKFace')\n",
    "print(\"\\n\")\n",
    "check_directory_contents('datasets/FER-2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6ea2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_utkface_dataset(utkface_dir, custom_dir):\n",
    "    print(f\"Processing UTKFace dataset from {utkface_dir}\")\n",
    "    utkface_subdir = os.path.join(utkface_dir, 'UTKFace')\n",
    "    if not os.path.exists(utkface_subdir):\n",
    "        print(f\"UTKFace subdirectory not found: {utkface_subdir}\")\n",
    "        return\n",
    "\n",
    "    processed_count = 0\n",
    "    for img_name in os.listdir(utkface_subdir):\n",
    "        if img_name.endswith('.jpg') or img_name.endswith('.png'):\n",
    "            try:\n",
    "                age, gender, race, _ = img_name.split('_')\n",
    "                age = int(age)\n",
    "                race = int(race)\n",
    "\n",
    "                if 10 <= age <= 60:\n",
    "                    if race == 1:\n",
    "                        nationality = 'american'\n",
    "                    elif race == 2:\n",
    "                        nationality = 'african'\n",
    "                    elif race == 4:\n",
    "                        nationality = 'indian'\n",
    "                    else:\n",
    "                        nationality = 'other'\n",
    "\n",
    "                    split = np.random.choice(['train', 'val', 'test'], p=[0.8, 0.1, 0.1])\n",
    "                    \n",
    "                    src_path = os.path.join(utkface_subdir, img_name)\n",
    "                    dst_path = os.path.join(custom_dir, split, nationality, img_name)\n",
    "                    shutil.copy(src_path, dst_path)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                    if processed_count % 1000 == 0:\n",
    "                        print(f\"Processed {processed_count} images from UTKFace\")\n",
    "            except ValueError:\n",
    "                print(f\"Skipping file with unexpected format: {img_name}\")\n",
    "\n",
    "    print(f\"UTKFace dataset processed. Total images copied: {processed_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f54c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fer2013_dataset(fer2013_dir, custom_dir):\n",
    "    print(f\"Processing FER-2013 dataset from {fer2013_dir}\")\n",
    "    if not os.path.exists(fer2013_dir):\n",
    "        print(f\"FER-2013 directory not found: {fer2013_dir}\")\n",
    "        return\n",
    "\n",
    "    processed_count = 0\n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = os.path.join(fer2013_dir, split)\n",
    "        for emotion in os.listdir(split_dir):\n",
    "            emotion_dir = os.path.join(split_dir, emotion)\n",
    "            for img_name in os.listdir(emotion_dir):\n",
    "                if img_name.endswith('.jpg') or img_name.endswith('.png'):\n",
    "                    nationality = np.random.choice(['indian', 'american', 'african', 'other'])\n",
    "                    custom_split = 'train' if split == 'train' else np.random.choice(['val', 'test'], p=[0.5, 0.5])\n",
    "\n",
    "                    src_path = os.path.join(emotion_dir, img_name)\n",
    "                    dst_path = os.path.join(custom_dir, custom_split, nationality, f\"{emotion}_{img_name}\")\n",
    "                    shutil.copy(src_path, dst_path)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                    if processed_count % 1000 == 0:\n",
    "                        print(f\"Processed {processed_count} images from FER-2013\")\n",
    "\n",
    "    print(f\"FER-2013 dataset processed. Total images copied: {processed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5a7a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_structure():\n",
    "    base_dir = 'custom_dataset'\n",
    "    nationalities = ['indian', 'american', 'african', 'other']\n",
    "    splits = ['train', 'val', 'test']\n",
    "\n",
    "    for split in splits:\n",
    "        for nationality in nationalities:\n",
    "            os.makedirs(os.path.join(base_dir, split, nationality), exist_ok=True)\n",
    "\n",
    "    print(\"Directory structure created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baa26cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation_csv(custom_dataset_dir, output_csv):\n",
    "    print(f\"Creating annotation CSV file: {output_csv}\")\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['image_path', 'nationality', 'emotion', 'age', 'dress_color'])\n",
    "\n",
    "        total_annotations = 0\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            for nationality in ['indian', 'american', 'african', 'other']:\n",
    "                img_dir = os.path.join(custom_dataset_dir, split, nationality)\n",
    "                for img_name in os.listdir(img_dir):\n",
    "                    img_path = os.path.join(split, nationality, img_name)\n",
    "                    \n",
    "                    if img_name.startswith(('angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise')):\n",
    "                        emotion = img_name.split('_')[0]\n",
    "                    else:\n",
    "                        emotion = np.random.choice(['happy', 'sad', 'angry', 'neutral', 'surprise'])\n",
    "                    \n",
    "                    if img_name.startswith(tuple(map(str, range(1, 117)))):\n",
    "                        age = int(img_name.split('_')[0])\n",
    "                    else:\n",
    "                        age = np.random.randint(10, 61)\n",
    "                    \n",
    "                    dress_color = np.random.choice(['red', 'blue', 'green', 'yellow', 'white', 'black'])\n",
    "\n",
    "                    csvwriter.writerow([img_path, nationality, emotion, age, dress_color])\n",
    "                    total_annotations += 1\n",
    "                    \n",
    "                    if total_annotations % 1000 == 0:\n",
    "                        print(f\"Created {total_annotations} annotations\")\n",
    "\n",
    "    print(f\"Annotation CSV file created. Total annotations: {total_annotations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d52d9df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure created successfully!\n",
      "Processing UTKFace dataset from datasets/UTKFace\n",
      "Processed 1000 images from UTKFace\n",
      "Processed 2000 images from UTKFace\n",
      "Processed 3000 images from UTKFace\n",
      "Processed 4000 images from UTKFace\n",
      "Processed 5000 images from UTKFace\n",
      "Processed 6000 images from UTKFace\n",
      "Processed 7000 images from UTKFace\n",
      "Processed 8000 images from UTKFace\n",
      "Processed 9000 images from UTKFace\n",
      "Processed 10000 images from UTKFace\n",
      "Processed 11000 images from UTKFace\n",
      "Processed 12000 images from UTKFace\n",
      "Processed 13000 images from UTKFace\n",
      "Skipping file with unexpected format: 39_1_20170116174525125.jpg.chip.jpg\n",
      "Processed 14000 images from UTKFace\n",
      "Processed 15000 images from UTKFace\n",
      "Processed 16000 images from UTKFace\n",
      "Processed 17000 images from UTKFace\n",
      "Processed 18000 images from UTKFace\n",
      "Skipping file with unexpected format: 61_1_20170109142408075.jpg.chip.jpg\n",
      "Skipping file with unexpected format: 61_1_20170109150557335.jpg.chip.jpg\n",
      "UTKFace dataset processed. Total images copied: 18248\n",
      "Processing FER-2013 dataset from datasets/FER-2013\n",
      "Processed 1000 images from FER-2013\n",
      "Processed 2000 images from FER-2013\n",
      "Processed 3000 images from FER-2013\n",
      "Processed 4000 images from FER-2013\n",
      "Processed 5000 images from FER-2013\n",
      "Processed 6000 images from FER-2013\n",
      "Processed 7000 images from FER-2013\n",
      "Processed 8000 images from FER-2013\n",
      "Processed 9000 images from FER-2013\n",
      "Processed 10000 images from FER-2013\n",
      "Processed 11000 images from FER-2013\n",
      "Processed 12000 images from FER-2013\n",
      "Processed 13000 images from FER-2013\n",
      "Processed 14000 images from FER-2013\n",
      "Processed 15000 images from FER-2013\n",
      "Processed 16000 images from FER-2013\n",
      "Processed 17000 images from FER-2013\n",
      "Processed 18000 images from FER-2013\n",
      "Processed 19000 images from FER-2013\n",
      "Processed 20000 images from FER-2013\n",
      "Processed 21000 images from FER-2013\n",
      "Processed 22000 images from FER-2013\n",
      "Processed 23000 images from FER-2013\n",
      "Processed 24000 images from FER-2013\n",
      "Processed 25000 images from FER-2013\n",
      "Processed 26000 images from FER-2013\n",
      "Processed 27000 images from FER-2013\n",
      "Processed 28000 images from FER-2013\n",
      "Processed 29000 images from FER-2013\n",
      "Processed 30000 images from FER-2013\n",
      "Processed 31000 images from FER-2013\n",
      "Processed 32000 images from FER-2013\n",
      "Processed 33000 images from FER-2013\n",
      "Processed 34000 images from FER-2013\n",
      "Processed 35000 images from FER-2013\n",
      "FER-2013 dataset processed. Total images copied: 35887\n",
      "Creating annotation CSV file: custom_dataset_annotations.csv\n",
      "Created 1000 annotations\n",
      "Created 2000 annotations\n",
      "Created 3000 annotations\n",
      "Created 4000 annotations\n",
      "Created 5000 annotations\n",
      "Created 6000 annotations\n",
      "Created 7000 annotations\n",
      "Created 8000 annotations\n",
      "Created 9000 annotations\n",
      "Created 10000 annotations\n",
      "Created 11000 annotations\n",
      "Created 12000 annotations\n",
      "Created 13000 annotations\n",
      "Created 14000 annotations\n",
      "Created 15000 annotations\n",
      "Created 16000 annotations\n",
      "Created 17000 annotations\n",
      "Created 18000 annotations\n",
      "Created 19000 annotations\n",
      "Created 20000 annotations\n",
      "Created 21000 annotations\n",
      "Created 22000 annotations\n",
      "Created 23000 annotations\n",
      "Created 24000 annotations\n",
      "Created 25000 annotations\n",
      "Created 26000 annotations\n",
      "Created 27000 annotations\n",
      "Created 28000 annotations\n",
      "Created 29000 annotations\n",
      "Created 30000 annotations\n",
      "Created 31000 annotations\n",
      "Created 32000 annotations\n",
      "Created 33000 annotations\n",
      "Created 34000 annotations\n",
      "Created 35000 annotations\n",
      "Created 36000 annotations\n",
      "Created 37000 annotations\n",
      "Created 38000 annotations\n",
      "Created 39000 annotations\n",
      "Created 40000 annotations\n",
      "Created 41000 annotations\n",
      "Created 42000 annotations\n",
      "Created 43000 annotations\n",
      "Created 44000 annotations\n",
      "Created 45000 annotations\n",
      "Created 46000 annotations\n",
      "Created 47000 annotations\n",
      "Created 48000 annotations\n",
      "Created 49000 annotations\n",
      "Created 50000 annotations\n",
      "Created 51000 annotations\n",
      "Created 52000 annotations\n",
      "Created 53000 annotations\n",
      "Created 54000 annotations\n",
      "Created 55000 annotations\n",
      "Created 56000 annotations\n",
      "Created 57000 annotations\n",
      "Created 58000 annotations\n",
      "Created 59000 annotations\n",
      "Created 60000 annotations\n",
      "Created 61000 annotations\n",
      "Created 62000 annotations\n",
      "Created 63000 annotations\n",
      "Created 64000 annotations\n",
      "Created 65000 annotations\n",
      "Created 66000 annotations\n",
      "Created 67000 annotations\n",
      "Created 68000 annotations\n",
      "Created 69000 annotations\n",
      "Created 70000 annotations\n",
      "Created 71000 annotations\n",
      "Created 72000 annotations\n",
      "Created 73000 annotations\n",
      "Created 74000 annotations\n",
      "Created 75000 annotations\n",
      "Created 76000 annotations\n",
      "Created 77000 annotations\n",
      "Created 78000 annotations\n",
      "Created 79000 annotations\n",
      "Created 80000 annotations\n",
      "Created 81000 annotations\n",
      "Created 82000 annotations\n",
      "Created 83000 annotations\n",
      "Created 84000 annotations\n",
      "Created 85000 annotations\n",
      "Created 86000 annotations\n",
      "Created 87000 annotations\n",
      "Created 88000 annotations\n",
      "Annotation CSV file created. Total annotations: 88219\n"
     ]
    }
   ],
   "source": [
    "# Run the processing steps\n",
    "create_directory_structure()\n",
    "process_utkface_dataset('datasets/UTKFace', 'custom_dataset')\n",
    "process_fer2013_dataset('datasets/FER-2013', 'custom_dataset')\n",
    "create_annotation_csv('custom_dataset', 'custom_dataset_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6703fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_custom_dataset(custom_dir):\n",
    "    print(f\"Checking contents of custom dataset: {custom_dir}\")\n",
    "    total_images = 0\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for nationality in ['indian', 'american', 'african', 'other']:\n",
    "            dir_path = os.path.join(custom_dir, split, nationality)\n",
    "            if os.path.exists(dir_path):\n",
    "                files = os.listdir(dir_path)\n",
    "                total_images += len(files)\n",
    "                print(f\"  {split}/{nationality}: {len(files)} images\")\n",
    "    print(f\"Total images in custom dataset: {total_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74493a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking contents of custom dataset: custom_dataset\n",
      "  train/indian: 13878 images\n",
      "  train/american: 16474 images\n",
      "  train/african: 14619 images\n",
      "  train/other: 22826 images\n",
      "  val/indian: 1942 images\n",
      "  val/american: 2410 images\n",
      "  val/african: 2148 images\n",
      "  val/other: 3668 images\n",
      "  test/indian: 1960 images\n",
      "  test/american: 2421 images\n",
      "  test/african: 2145 images\n",
      "  test/other: 3728 images\n",
      "Total images in custom dataset: 88219\n",
      "\n",
      "\n",
      "Checking contents of CSV file: custom_dataset_annotations.csv\n",
      "  CSV header: ['image_path', 'nationality', 'emotion', 'age', 'dress_color']\n",
      "  Total rows (excluding header): 88219\n"
     ]
    }
   ],
   "source": [
    "def check_csv_file(csv_path):\n",
    "    print(f\"Checking contents of CSV file: {csv_path}\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"  CSV file does not exist!\")\n",
    "        return\n",
    "    \n",
    "    with open(csv_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        header = next(csvreader, None)\n",
    "        print(f\"  CSV header: {header}\")\n",
    "        \n",
    "        row_count = sum(1 for row in csvreader)\n",
    "        print(f\"  Total rows (excluding header): {row_count}\")\n",
    "\n",
    "check_custom_dataset('custom_dataset')\n",
    "print(\"\\n\")\n",
    "check_csv_file('custom_dataset_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461945c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c49156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6652d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
